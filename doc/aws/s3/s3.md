## Amazon S3
### 1. Amazon S3
#### 1. Section introduction
- Amazon S3 is one of the main building blocks of AWS  
→ `Amazon S3`는 `AWS`의 주요 구성 요소 중 하나이다.

- It’s advertised as ”infinitely scaling” storage  
→ 광고에서는 `S3`를 무한하게 확장 가능한 스토리지로 소개한다.

- Many AWS services use Amazon S3 as an integration as well  
→ 많은 `AWS` 서비스가 `Amazon S3`를 통합해서 사용한다.

#### 2. Amazon S3 Use cases
- 본질적으로 `S3`는 스토리지이기 때문에 많은 부분에서 사용된다. 

- Backup and storage  
→ 백업과 스토리지에 활용된다. 이는 파일용일 수도 있고 디스크용일 수도 있다.

- Disaster Recovery  
→ 재해 복구의 용도로 사용된다.

- Archive  
→ 아카이브용으로 사용된다. `Amazon S3`에 파일을 아카이브함으로써 추후 매우 손쉬운 검색이 가능하다.

- Hybrid Cloud storage  
→ 온프레미스 스토리지를 클라우드로 확장하고 싶을 때 사용된다.

- Application hosting  
→ 애플리케이션의 호스팅에 사용된다.

- Media hosting  
→ 동영상 파일이나 이미지 등 미디어의 호스팅에 사용된다.

- Data lakes & big data analytics  
→ 다량의 데이터를 저장하거나 빅데이터 분석을 수행할 때 사용된다.

- Static website  
→ 정적 웹사이트의 호스팅에 사용된다.

### 2. Amazon S3 - Buckets and Object
#### 1. Buckets
- Amazon S3 allows people to store objects (files) in “buckets” (directories)  
→ `Amazon S3`는 파일을 버킷에 저장하며 버킷은 상위 레벨 디렉토리로 표시된다. 버킷 내의 파일을 객체라고 부른다.

- Buckets must have a globally unique name (across all regions all accounts)  
→ 이 버킷은 계정 안에 생성된다. 버킷에는 `AWS`에 존재하는 모든 계정과 리전에서 전역적으로 고유해야 한다.

- Buckets are defined at the region level  
→ 버킷은 반드시 특정 `AWS` 리전에서 정의되어야 한다.

- S3 looks like a global service but buckets are created in a region  
→ `Amazon S3`는 전역 서비스처럼 보이지만 버킷은 사실상 리전에서 생성된다.

- Naming convention  
→ `Amazon S3` 버킷에는 명명규칙이 존재한다. 기본적으로 문자, 숫자, 하이픈만 사용하면 된다.
~~~
- No uppercase, No underscore
- 3-63 characters long
- Not an IP
- Must start with lowercase letter or number
- Must NOT start with the prefix xn--
- Must NOT end with the suffix -s3alias
~~~

#### 2. Objects
- Objects (files) have a Key  
→ 객체나 파일에는 키가 존재한다.

- The key is the FULL path:  
→ `Amazon S3`의 키는 파일의 전체 경로이다.
~~~
- s3://my-bucket/my_file.txt
→ my_file.txt가 키가 된다.

- s3://my-bucket/my_folder1/another_folder/my_file.txt
→ my_folder1/another_folder/my_file.txt가 키가 된다.
~~~

- The key is composed of prefix + object name  
→ 키는 접두사와 객체 이름으로 구성되어 있다.
~~~
- s3://my-bucket/my_folder1/another_folder/my_file.txt
→ my_folder1/another_folder/my_file.txt를 접두사인 my_folder1/another_folder/와 객체 이름인 my_file.txt로 분해할 수 있다.
~~~

- There’s no concept of “directories” within buckets (although the UI will trick you to think otherwise)  
→ `Amazon S3` 자체에 디렉토리의 개념은 없다. 

- Just keys with very long names that contain slashes (“/”)  
→ 키는 굉장히 긴 이름으로 슬래시를 포함하며, 두사와 객체 이름의 조합으로 만들어진다.

- Object values are the content of the body:  
→ 값은 본문의 내용이다. 파일 등 원하는 것은 뭐든지 `Amazon S3`로 업로드할 수 있다. 
~~~
- Max. Object Size is 5TB (5000GB)
→ 객체 최대 크기는 5TB이다.

- If uploading more than 5GB, must use “multi-part upload”
→ 만약 파일이 5GB보다 크다면 이때는 멀티파트 업로드를 사용해서 해당 파일을 여러 부분으로 나눠 업로드해야 한다.
~~~

- Metadata (list of text key / value pairs – system or user metadata)  
→ 객체에는 메타데이터도 존재한다. 이는 객체의 키-값 쌍의 리스트를 말한다. 이는 시스템이나 사용자에 의해 설정되어 파일에 관한 요소나 메타데이터를 나타낼 수 있다.

- Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle  
→ 태그의 경우 키-쌍 값을 최대 10개까지 가질 수 있으며 보안과 수명 주기에 유용하다.

- Version ID (if versioning is enabled)  
→ 버전 관리 활성화시 사용 가능하다.

#### 3. Amazon S3 - Create Bucket
- `Amazon S3` → `Buckets` → `Create Bucket`
- 버킷의 이름과 리전을 선택한다. 버킷 이름은 고유해야 한다. 버킷이 전역적이지 않다는 사실은 중요하다.

![image](https://user-images.githubusercontent.com/97398071/234624107-335aa4d9-1a30-4067-9046-ff496c8d0c99.png)

- 버킷 생성 후 어떤 파일이든 업로드할 수 있다. 업로드한 파일을 클릭하여 `Open` 버튼을 클릭하면 해당 파일이 웹 상에 존재하는 것을 확인할 수 있다.  
→ 이 때 생성되는 `URL`은 `S3`의 미리 서명된 `URL`로 쿼리 파라미터 내에 자격 증명에 관한 정보가 인코딩되어 있다.

- `Open` 버튼을 사용하지 않고 `Object URL`을 직접 입력할 경우 접근이 불가능할수도 있다. 그렇다면 공개 `URL`에 대한 접근을 허용하지 않은 것이다.

### 2. Amazon S3 - Security
- User-Based  
→ 사용자 기반일 경우 사용자에게 특정 `IAM` 정책이 주어진다. 이 정책은 어떤 `API` 호출이 특정 `IAM` 사용자를 위해 허용되어야 하는지를 승인한다.
~~~
- IAM Policies – which API calls should be allowed for a specific user from IAM
~~~

- Resource-Based  
→ 리소스를 보안 기반으로 사용할 수도 있다. 가장 일반적인 방법은 버킷 정책을 사용하는 것이다.
~~~
- Bucket Policies – bucket wide rules from the S3 console - allows cross account
→ S3 버킷 정책을 사용할 수 있다. 이 정책은 S3 콘솔에서 직접 할당할 수 있는 전체 버킷 규칙이다.

- Object Access Control List (ACL) – finer grain (can be disabled)
→ 객체 액세스 제어 목록이다. 이 목록은 보다 세밀한 보안이며 비활성화가 가능하다.

- Bucket Access Control List (ACL) – less common (can be disabled)
→ 버킷 수준에서 살펴보면 버킷 ACL이 있는데 훨씬 덜 일반적이다. 이 역시 비활성화할 수 있다.
~~~

- Note: an IAM principal can access an S3 object if  
→ `IAM` 권한 또는 리소스 정책을 허용하는 경우에만 `S3` 객체에 접근할 수 있다.
~~~
- The user IAM permissions ALLOW it OR the resource policy ALLOWS it
- AND there’s no explicit DENY
~~~

- S3 버킷 정책을 업데이트해 `IAM` 사용자들이 `S3` 버킷 내의 파일을 읽기/쓰기할 수 있도록 허가했으나 한 명의 사용자가 `PutObject API` 호출을 수행할 수 없다며 불만을 토로하고 있습니다. 이 경우, 가능성이 있는 원인은 무엇일까요?  
→ 연결된 `IAM` 정책 내에 이 사용자를 부인하는 사항이 명시되어 있음. `IAM` 정책 내의 명시적인 부인(DENY)은 `S3` 버킷 정책보다 우선적으로 고려된다.

- Encryption: encrypt objects in Amazon S3 using encryption keys  
→ 암호키를 사용해 객체를 암호화할 수 있다. 

#### 1. S3 Bucket Policies
- `S3` 보안 설정에 사용되는 `JSON` 기반의 정책이다. 
- JSON based policies
→ `Resource` 블럭은 여러 개 선언이 가능하다.
~~~
- Resources: buckets and objects
→ 이 정책이 적용되는 버킷과 객체를 선택한다.

- Effect: Allow / Deny
→ 작업을 허용할지 거부할지 여부를 선택한다.

- Actions: Set of API to Allow or Deny
→ 허용하거나 거부할 수 있는 API의 집합을 설정한다.

- Principal: The account or user to apply the policy to
→ 원칙을 통해 허용할 사용자를 설정한다. 아스타리스크를 사용하면 모든 사용자를 허용한다.
~~~

![image](https://user-images.githubusercontent.com/97398071/234629633-88b8c385-afcf-4983-8e9e-3f26bbfb1033.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- Use S3 bucket for policy to:
~~~
- Grant public access to the bucket
→ 버킷에 대한 공개 액세스를 허용할 수 있다.

- Force objects to be encrypted at upload
→ 업로드 시 객체를 강제로 암호화할 수 있다.

- Grant access to another account (Cross Account)
→ 다른 계정으로의 액세스를 허용할 수 있다.
~~~

#### 2. Bucket Policies - Use Case
- 정책 설정을 통해 모든 유저에게의 공개 액세스를 허용할 수 있다.
- 정책 설정을 통해 `IAM` 유저에게 버킷 접근 권한을 부여하는 것으로 특정 `IAM` 유저에게 액세스를 허용할 수 있다.
- `EC2` 인스턴스가 있는 경우 `EC2` 인스턴스에서 `S3` 버킷으로의 액세스를 허용할 수 있다. 이 때에는 `IAM Role`을 활용해야 한다.
- 다른 계정에 있는 `IAM` 사용자에게 교차 계정 액세스를 허용하려면 버킷 정책을 사용해야 한다. 설정을 통해 `S3` 버킷으로 `API` 호출이 가능해진다.

#### 3. Bucket settings for Block Public Access
- These settings were created to prevent company data leaks  
→ 기업 데이터 유출을 방지하기 위한 추가 계층으로써 `AWS`가 개발했다.

- If you know your bucket should never be public, leave these on  
→ `S3` 버킷 정책을 설정하여 공개로 만들더라도 이 설정이 활성화되어 있다면 버킷은 결코 공개되지 않는다.
버킷을 공개하면 안 되는 경우, 이 설정을 그대로 두면 잘못된 `S3` 버킷 정책을 설정한 사람들에 대해 보안을 갖출 수 있다.

- Can be set at the account level  
→ `S3` 버킷 중 어느 것도 공개되서는 안 된다면 계정 수준에서 이를 설정하면 된다.

#### 4. Update Bucket Permission
- `Amazon S3` → `Buckets` → 버킷 선택 → `Permission`

- `Block all public access`  
→ 이 부분을 선택 해제하면 공개 액세스를 허용할 수 있다. 공개 버킷 정책을 설정하려는 경우에만 비활성화하도록 하자. 이 작업은 위험하다.

- `Edit bucket policy`  
→ 버킷 정책을 변경할 수 있다.

![image](https://user-images.githubusercontent.com/97398071/234633374-51273174-c59a-4a99-9095-3d7b1ad1d805.png)

- [버킷 정책 생성기](https://awspolicygen.s3.amazonaws.com/policygen.html)를 통해 정책을 쉽게 작성할 수 있다.  
→ `ARN, Amazon Resource Name`을 입력할 때 맨 뒤에 `/*`를 추가해야 한다.

![image](https://user-images.githubusercontent.com/97398071/234634288-4cd7736f-fab5-4ef5-9d09-78908ffe0d0d.png)

- [정책 예제](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/example-bucket-policies.html)에서 여러 정책 예제를 확인할 수 있다.

![image](https://user-images.githubusercontent.com/97398071/234633661-3cd693c0-e66b-4217-9725-b8f02ab4a2d5.png)

### 3. Amazon S3 – Static Website Hosting
- S3 can host static websites and have them accessible on the Internet  
→ `S3`를 통해 웹사이트를 호스팅할 수 있다. 호스팅한 웹사이트의 액세스 또한 가능하다.

- The website URL will be (depending on the region)  
→ 웹 사이트 `URL`은 생성하는 `AWS` 리전에 따라 달라진다. 아래 두 개의 `URL`간의 차이점은 `aws` 앞의 `-`와 `.`이다. 굳이 기억하지 않아도 괜찮다.
~~~
- http://bucket-name.s3-website-aws-region.amazonaws.com
OR
- http://bucket-name.s3-website.aws-region.amazonaws.com
~~~

- If you get a 403 Forbidden error, make sure the bucket policy allows public reads!  
→ 버킷의 공개 읽기가 활성화되지 않았다면 작동하지 않는다.

- `S3` → `Bucket` → `Properties` → `static website hosting`  
→ `Hosting type`을 `Host a static website`로 변경하고 `Index document`를 설정해야 한다.

- 저장 후 `Objects`로 돌아가서 `Index document`에 설정한 `html` 파일을 업로드한다.

- `Static website hosting` 항목에서 `URL`을 복사하고 실행하면 정적 웹사이트의 작동을 확인할 수 있다.

### 4. Amazon S3 - Versioning
- You can version your files in Amazon S3 
→ `Amazon S3`는 파일 버전 관리를 지원한다.

- It is enabled at the bucket level  
→ 파일 버전 관리는 버킷 수준에서 활성화해야 하는 설정이다.

- Same key overwrite will change the “version”: 1, 2, 3….  
→ 동일한 키를 업로드하고 해당 파일을 덮어쓰는 경우 버전 2, 버전 3 등을 생성하게 된다.

- It is best practice to version your buckets  
→ 버전을 관리하도록 설정하는 것이 추천된다.
~~~
- Protect against unintended deletes (ability to restore a version)
→ 이 방법은 의도치 않은 삭제에서부터 파일을 보호해준다.

- Easy roll back to previous version
→ 이전 버전으로의 복구가 가능하다.
~~~
- Notes:
~~~
- Any file that is not versioned prior to enabling versioning will have version “null”
→ 버전 관리를 활성화하기 전의 버전 관리가 적용되지 않은 모든 파일은 널 버전을 가진다.

- Suspending versioning does not delete the previous versions
→ 버전 관리를 중단해도 이전 버전을 삭제하지는 않는다.
~~~

- `S3` → `Bucket` → `Properties` → `Bucket Versioning` → `Edit`  
→ `Bucket Versioning`을 활성화 할 수 있다.

- 특정 버전의 파일을 삭제하면 이 작업을 되돌릴 수 없다.

- 그러나, 버전 `ID`가 널인 파일을 삭제할 경우에는 `Delete Marker`가 있는 파일로 덮어씌워진다. 이 `Delete Marker`를 삭제하면 파일의 복원이 가능하다.

### 5. Amazon S3 – Replication (CRR & SRR)
- Must enable Versioning in source and destination buckets  
→ 복제를 위해서는 가장 먼저 소스 버킷과 복제 대상 버킷의 버전 관리 기능을 활성화해야 한다.

- 아마존 복제에는 두 가지 종류가 있다. 
~~~ 
- Cross-Region Replication (CRR)
→ 교차 리전으로 복제, 소스 버킷과 복제 대상 버킷 리전이 달라야 한다.

- Same-Region Replication (SRR)
→ 같은 리전으로 복제, 소스 버킷과 복제 대상 버킷 리전이 같아야 한다.
~~~

- Buckets can be in different AWS accounts  
→ 버킷은 서로 다른 `AWS` 계정간에도 사용할 수 있다.

- Copying is asynchronous  
→ 복제는 비동기식으로 이루어지며 백그라운드에서 실행된다.

- Must give proper IAM permissions to S3  
→ 복제 기능이 정상적으로 실행되려면 `S3`에 올바른 `IAM` 권한, 즉 읽기와 쓰기 권한을 `S3`에 부여해야 한다.

- Use cases:
~~~
- CRR – compliance, lower latency access, replication across accounts
→ 법규나 내부 체제를 관리하거나 데이터가 다른 리전에 있어 발생할 수 있는 지연 시간을 줄일 목적으로 사용한다.

- SRR – log aggregation, live replication between production and test accounts
→ 다수의 S3 버킷간의 로그를 통합할 때 개발 환경이 별도로 존재해 운영 환경과 개발 환경간의 실시간 복제를 필요로 할 때 사용한다.
~~~

- After you enable Replication, only new objects are replicated  
→ 복제를 활성화한 후에는 새로운 객체만 복제 대상이 된다.

- Optionally, you can replicate existing objects using S3 Batch Replication  
→ 기존의 객체를 복제하려면 `S3 배치 복제, S3 Batch Replication` 기능을 사용해야 한다.
~~~
- Replicates existing objects and objects that failed replication
→ 기존 객체부터 복제에 실패한 객체까지 복제할 수 있는 기능이다.
~~~

- For DELETE operations
~~~
- Can replicate delete markers from source to target (optional setting)
→ 소스 버킷에서 대상 버킷으로 삭제 마커를 복제할 수 있다.

- Deletions with a version ID are not replicated (to avoid malicious deletes)
→ 버전 ID가 존재하는 객체를 삭제하는 경우에는 버전 ID가 복제되지 않는다. 이는 영구적인 삭제이다. 악의적인 사용자가 한 버킷에서 다른 버킷으로 삭제 마커를 복제하면 안 되기 때문이다.
~~~

- There is no “chaining” of replication  
→ 체이닝 복제는 불가능하다.

- If bucket 1 has replication into bucket 2, which has replication into bucket 3, Then objects created in bucket 1 are not replicated to bucket 3  
→ 1번 버킷이 2번 버킷에 복제되어 있고 2번 버킷이 3번 버킷에 복제되어 있다고 해서 1번 버킷의 객체가 3번 버킷으로 복제되지는 않는다.

- `S3` 버킷이 3개 있습니다. 하나는 원본 버킷 `A`이고 나머지 둘은 대상 버킷 `B`와 `C`입니다. 버킷 A에 있는 객체를 버킷 `B`와 `C` 양쪽 모두에 복제하려고 합니다. 어떻게 구성해야 합니까?
→ 버킷 `A`에서 버킷 `B`로 복제하고, 버킷 `A`에서 버킷 `C`로 복제하도록 설정한다.

- `S3` → `Bucket` → `Origin Bucket` 선택 → `Management` → `Replication rules`  
→ 복제 규칙을 생성할 수 있다. `Destination`에 복제에 사용할 버킷을 설정한다. 설정을 완료하면 활성화된 시점 이후에 새로 업로드되는 객체만 복제된다.

- 대상 버킷의 소스 버킷에서 복제 활성화 이전의 객체를 복사하려면 `S3` 배치 작업 처리를 실행해야 한다.

- 복제 설정 시 `Delete marker replication` 옵션이 있다. 기본적으로 삭제 마커는 복제되지 않지만 복제되도록 설정하는 기능이다. 중요하다.

- 오리진 버킷에서 영구 객체를 삭제해도 복제 버킷에서는 삭제되지 않는다.

### 6. S3 Storage Classes
- Amazon S3 Standard - General Purpose
- Amazon S3 Standard-Infrequent Access (IA)
- Amazon S3 One Zone-Infrequent Access
- Amazon S3 Glacier Instant Retrieval
- Amazon S3 Glacier Flexible Retrieval
- Amazon S3 Glacier Deep Archive
- Amazon S3 Intelligent Tiering
- Can move between classes manually or using S3 Lifecycle configurations  
→ `Amazon S3`에서 객체를 생성할 때 클래스를 선택하거나 스토리지 클래스를 수동으로 수정할 수 있다.
혹은 `Amazon S3` 수명 주기 구성을 사용해 스토리지 클래스 간에 객체를 자동으로 이동시킬 수도 있다.

- 모든 수치를 기억할 필요는 없다. 각 클래스가 무엇인지만 이해하면 된다.

![image](https://user-images.githubusercontent.com/97398071/234881770-080bd3a1-5c62-4689-8ec8-eb85458f6c67.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- 버킷의 스토리지 클래스 변경을 자동화할 수 있다. `S3` → `Bucket` → `Bucket` 선택 → `Management` → `Lifecycle rules`를 활용하는 것이다.

#### 1. S3 Durability and Availability
- Durability:
~~~
- High durability (99.999999999%, 11 9’s) of objects across multiple AZ
→ 내구성은 Amazon S3로 인해 객체가 손실되는 횟수를 나타낸다. 그리고 Amazon S3는 매우 뛰어난 내구성을 제공한다. 

- If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years
→ Amazon S3에 1000만개의 객체를 저장했을 때, 평균적으로 10000년에 한 번 객체 손실이 예상된다.

- Same for all storage classes
→ 모든 스토리지 클래스의 내구성은 동일하다.
~~~

- Availability:
~~~
- Measures how readily available a service is
→ 가용성은 서비스가 얼마나 용이하게 제공되는지를 나타낸다.

- Varies depending on storage class
→ 이는 스토리지 클래스별로 다르다.

- Example: S3 standard has 99.99% availability = not available 53 minutes a year
→ 예를 들어 S3 Standard의 가용성은 99.99%인데 1년에 약 53분 동안은 서비스를 사용할 수 없다는 의미이다.
그러므로 애플리케이션을 개발할 때 이 부분을 고려해야 한다.
~~~

#### 2. S3 Standard – General Purpose
- 99.99% Availability
- Used for frequently accessed data
→ 자주 액세스하는 데이터에 사용되는 기본적으로 사용되는 스토리지 유형이다.

- Low latency and high throughput
→ 지연 시간이 짧고 처리량이 높다.

- Sustain 2 concurrent facility failures
→ `AWS`에서 두 개의 기능 장애를 동시에 버틸 수 있다.

- Use Cases: Big Data analytics, mobile & gaming applications, content distribution…
→ 사용 사례에는 빅데이터 분석, 모바일과 게임 애플리케이션, 콘텐츠 배포 등이 있다.

#### 3. S3 Storage Classes – Infrequent Access
- For data that is less frequently accessed, but requires rapid access when needed  
→ 자주 액세스하지 않지만 필요한 경우 빠르게 액세스해야 할 경우에 사용하는 스토리지 유형이다..

- Lower cost than S3 Standard  
→ `S3 Standard`보다 비용이 적게 들지만 검색 비용이 발생한다.

- Amazon S3 Standard-Infrequent Access (S3 Standard-IA)
~~~
- 99.9% Availability
→ `S3 Standard`보다 약간 떨어진다.

- Use cases: Disaster Recovery, backups
→ 사용 사례에는 재해 복구와 백업이 있다.
~~~

- Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)  
→ `One Zone-IA`로도 불린다.
~~~
- High durability (99.999999999%) in a single AZ; data lost when AZ is destroyed
→ 단일 AZ 내에서는 높은 내구성을 갖지만 AZ가 파괴된 경우 데이터를 잃게 된다.

- 99.5% Availability
→ 가용성은 더 낮은 수준인 99.5%이다.

- Use Cases: Storing secondary backup copies of on-premises data, or data you can recreate
→ 온프레미스 데이터를 2차 백업하거나 재생성 가능한 데이터를 저장하는 데 사용된다.
~~~

#### 4. Amazon S3 Glacier Storage Classes
- 이름에서 알 수 있듯이 콜드 스토리지이다.
- Low-cost object storage meant for archiving / backup  
→ 아카이빙과 백업을 위한 저비용 객체 스토리지이다.

- Pricing: price for storage + object retrieval cost  
→ 비용에는 스토리지 비용과 검색 비용이 들어간다.

- `Glacier` 스토리지에는 3가지 종류가 존재한다.

- Amazon S3 Glacier Instant Retrieval
~~~
- Millisecond retrieval, great for data accessed once a quarter
→ 밀리초 단위로 검색이 가능하다. 백업이지만 밀리초 이내에 액세스해야 하는 경우 적합하다. 예를 들어 분기에 한 번 액세스하는 데이터에 아주 적합하다.

- Minimum storage duration of 90 days
→ 최소 보관 기간이 90일이다. 이는 스토리지에 최소한 90일간 보관해야한다는 의미이다.
 ~~~

- Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):
~~~
- Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free
→ 세 가지 옵션이 있다. Expedited는 데이터를 1~5분 이내에 받을 수 있고 Standard는 3~5시간이 소요된다. Bulk는 무료지만 데이터를 돌려받는 데 5~12시간이 소요된다.

- Minimum storage duration of 90 days
→ 최소 보관 기간이 90일이다.
~~~

- Amazon S3 Glacier Deep Archive – for long term storage:
~~~
- Standard (12 hours), Bulk (48 hours)
→ 장기 보관을 위한 아카이브이다. Standard는 12시간이, Bulk는 48시간이 걸린다.

- Minimum storage duration of 180 days
→ 오래 걸리긴 하지만 비용이 가장 저렴하다. 그리고 최소 보관 기간도 180일이다.
~~~

#### 5. S3 Intelligent-Tiering
- 알아서 객체를 이동시켜주기 때문에 편하게 스토리지를 관리할 수 있다.

- Small monthly monitoring and auto-tiering fee  
→ 소액의 월별 모니터링 비용과 티어링 비용이 발생한다.

- Moves objects automatically between Access Tiers based on usage  
→ 사용 패턴에 따라 액세스된 티어 간에 객체를 이동시킨다.

- There are no retrieval charges in S3 Intelligent-Tiering  
→ S3 Intelligent-Tiering는 검색 비용이 없다.

- Frequent Access tier (automatic): default tier  
→ 자동이고 기본 티어이다.  

- Infrequent Access tier (automatic): objects not accessed for 30 days  
→ 30일 동안 액세스하지 않은 객체의 전용 티어이다.

- Archive Instant Access tier (automatic): objects not accessed for 90 days  
→ 자동이지만 90일동안 액세스하지 않은 객체의 전용 티어이다.

- Archive Access tier (optional): configurable from 90 days to 700+ days  
→ 선택 사항이며 90일에서 700일 이상까지 구성할 수 있다.

- Deep Archive Access tier (optional): config. from 180 days to 700+ days  
→ 선택 사항이며 180일에서 700일 이상 액세스하지 않은 객체에 대해 구성할 수 있다.

#### 6. Moving between Storage Classes
- You can transition objects between storage classes  
→ 스토리지 클래스 간에 객체를 이동시킬 수 있다. 자신보다 아래에 있는 모든 수즌으로 이동할 수 있다.

![image](https://user-images.githubusercontent.com/97398071/235154399-94212fc6-c0e8-4d23-8e73-372b0bb6cc1b.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- Moving objects can be automated using a Lifecycle Rules  
→ 객체는 수동으로 옮기거나 수명 주기 규칙을 이용해서 자동화할 수 있다.

#### 7. Lifecycle Rules
- Transition Actions – configure objects to transition to another storage class  
→ 전환 작업은 다른 스토리지 클래스로 객체를 전환하도록 구성한다. 예를 들면 아래와 같다.
~~~
- Move objects to Standard IA class 60 days after creation
→ 생성 60일 이후에 Standard IA로 이동하도록 구성한다.

- Move to Glacier for archiving after 6 months
→ 6개월 후에 Glacier에 아카이빙되도록 구성한다.
~~~

- Expiration actions – configure objects to expire (delete) after some time  
→ 만료 작업을 설정하면 일정 시간이 지난 후 객체가 삭제 또는 만료되게 할 수 있다.
~~~
- Access log files can be set to delete after a 365 days
→ 365일 후 액세스 로그 파일을 삭제하도록 구성한다.

- Can be used to delete old versions of files (if versioning is enabled)
→ 버저닝을 활성화한 경우 이전 버전의 파일을 삭제하도록 구성한다.

- Can be used to delete incomplete Multi-Part uploads
→ 완료되지 않은 멀티파트 업로드를 삭제하도록 구성한다.
~~~

- Rules can be created for a certain prefix (example: s3://mybucket/mp3/*)  
→ 특정 접두사를 사용하여 전체 버킷 또는 버킷의 일부 경로에 규칙을 적용할 수 있다.

- Rules can be created for certain objects Tags (example: Department: Finance)  
→ 객체의 태그에 규칙을 적용할 수 있다. 

- `Amazon S3`에 업로드되는 프로필 사진의 이미지 섬네일을 생성하려고 한다. 섬네일은 원본 사진에서 재생성하기 쉬우므로 60일 동안만 보관한다. 소스 이미지는 60일 동안은 바로 검색할 수 있어야 하고 이후에는 사용자가 6시간까지 기다릴 수 있다. 어떻게 설계하면 되는가?  
→ `S3` 소스 이미지를 `Standard` 클래스에 두고 수명 주기 구성을 한 다음 60일 이후에 `Glacier`로 전환하면 된다. 
섬네일 이미지를 생성할 때 접두사를 이용해서 섬네일과 소스 이미지를 쉽게 구분할 수 있다.
섬네일은 `One-Zone IA`에 둘 수 있는데 이는 해당 이미지 액세스가 많지 않고 재생성하기 쉽기 때문이다. 그리고 수명 주기 구성을 통해 60일 이후 만료 또는 삭제시킨다.

- `S3` 삭제 후 30일 이내에는 즉시 복구가 가능해야 한다. 그로부터 365일 이내에는 해당 객체를 48시간 내에 복구할 수 있어야 한다.  
→ `S3` 버저닝을 활성화하여 객체 버전을 유지 및 보유하고 있어야 삭제 마커 뒤에 숨어 있는 삭제된 객체들을 복구할 수 있다.
그리고 객체 이전 버전을 `Standard AI`로 이동시키는 전환 규칙을 생성해야 한다. 그런 다음 이전 버전을 `Glacier Deep Archive`로 보내 아카이브하는 규칙이 있어야 한다.

#### 8. Amazon S3 Analytics – Storage Class Analysis
- Help you decide when to transition objects to the right storage class  
→ 한 클래스에서 다른 클래스로 객체를 전환하는 데에 있어 최적의 기간은 `Amazon S3 Analytics`를 이용해서 결정하면 된다.

- Recommendations for Standard and Standard IA  
→ `Standard`와 `Standard IA`용의 권장 사항을 제공한다.

- Does NOT work for One-Zone IA or Glacier  
→ `One-Zone IA`나 `Glacier`은 해당하지 않는다.

- Report is updated daily  
→ `S3` 버킷에서 `S3 Analytics`를 실행하면 버킷에 대한 권장 사항과 통계를 `CSV` 파일로 제공한다. 이 보고서는 매일 업데이트된다.

- 24 to 48 hours to start seeing data analysis  
→ 해당 데이터의 분석 결과까지 24시간에서 48시간이 소요된다.

- Good first step to put together Lifecycle Rules (or improve them)!  
→ 이 `CSV` 보고서로 수명 주기 규칙을 알아보고 개선 방향을 생각해 볼 수 있다.

- `Management` → `Create lifecycle rule`에서 생성 가능하다.

### 7. S3 – Requester Pays
- In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket  
→ 기본적으로 버킷 소유자가 버킷과 관련된 모든 `Amazon S3` 스토리지 및 데이터 전송 비용을 지불한다.

![image](https://user-images.githubusercontent.com/97398071/235160806-aac0e681-931a-4ad1-b262-e8fc0d840ce7.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket  
→ 수많은 대형 파일이 있을때 일부 고객이 이를 다운로드하려고 한다면 요청자 지불 버킷을 활성화하는 것이 좋다. 이 경우 버킷의 소유자가 아닌 요청자가 객체 데이터 다운로드 비용을 지불한다. 

- Helpful when you want to share large datasets with other accounts  
→ 대량의 데이터셋을 다른 계정과 공유하려고 할 때 매우 유용하다.

- The requester must be authenticated in AWS (cannot be anonymous)  
→ 요청자가 익명이어서는 안된다. 요청자는 `AWS`에서 인증을 받아야 한다.

### 8. S3 Event Notifications
- S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication…  
→ `S3`에서는 이벤트가 발생한다. 이 이벤트란 객체의 생성, 삭제, 복원, 복제 중 하나의 사건이 발생한 상황 의미한다.

- Use case: generate thumbnails of images uploaded to S3  
→ `Amazon S3`에서 발생하는 특정 이벤트에 자동으로 반응하게 할 수 있다. 사진의 섬네일을 생성하고자 한다면 이에 대한 알람을 만들어서 몇 가지 수신지로 보낼 수 있다.

- Can create as many “S3 events” as desired  
→ `S3` 이벤트를 생성하고 원하는 대상으로 보낼 수 있다. 대상에는 `SNS`, `SQS`, `Lambda Function`, `EventBridge`가 포함된다.

- S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer  
→ 이벤트가 대상에 전달되는 시간은 일반적으로 몇 초밖에 안 걸리지만 가끔은 1분 이상 걸릴 수도 있다.

![image](https://user-images.githubusercontent.com/97398071/235161881-2c241801-bd6c-46f9-b784-df173bea87ab.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- `Properties` → `Event notifications`에서 설정 가능하다. 이벤트 알림을 생성하거나 `Amazon EventBridge` 통합을 활성화할 수 있다.

- 이 때 `S3`가 대상에 메시지를 게시할 수 있도록 권한을 제공해야 한다.  
→ `SQS`에 이벤트 알림을 보낸다고 가정할 때, 생성한 `SQS`의 액세스 정책을 `S3`에서의 접근을 허용하도록 변경해야 한다.

#### 1. S3 Event Notifications with Amazon EventBridge
- 이벤트 알람이 새로운 기능으로 `Amazon EventBridge`와 통합되었다. 이벤트 종류와 상관없이 모든 이벤트는 `Amazon EventBridge`로 모이게 된다.

- `Amazon EventBridge`에서 설정한 규칙을 통해 18개가 넘는 `AWS` 서비스에 이벤트 알림을 보낼 수 있다. `S3 이벤트 알림` 기능을 크게 향상시켜 준다.

- Advanced filtering options with JSON rules (metadata, object size, name...)  
→ `Amazon EventBridge`가 있으면 고급 필터링 옵션을 이전보다 훨씬 더 많이 사용할 수 있다.

- Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose…  
→ 동시에 여러 수신지로 전송할 수 있다.

- EventBridge Capabilities – Archive, Replay Events, Reliable delivery  
→ `Amazon EventBridge`에서 제공하는 기능을 사용할 수도 있다. 이벤트를 보관하거나 재생할 수 있고 보다 안정적으로 전송할 수 있다.

### 9. S3 – Baseline Performance
- Amazon S3 automatically scales to high request rates, latency 100-200 ms  
→ 기본적으로 `S3`는 요청이 아주 많을 때 자동으로 확장된다. `S3`로부터 첫 번째 바이트를 수신하는 데 지연 시간도 100~200 밀리초 사이로 아주 짧다.

- Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket.  
→ `S3`는 버킷 내에서 접두사당 초당 3500개의 `PUT/COPY/POST/DELETE`, 초당 5500개의 `GET/HEAD` 요청을 지원한다. 굉장히 고성능이라는 뜻이다.

- There are no limits to the number of prefixes in a bucket.  
→ 버킷 내의 접두사 수에는 제한이 없다.

- Example (object path => prefix):  
→ `file`이라는 이름을 가진 네 개의 객체가 있다고 가정한다.
~~~
- bucket/folder1/sub1/file => /folder1/sub1/
→ `bucket`과 `file` 사이에 있는 것이 접두사가 된다. 이 접두사를 갖는 파일은 초당 3500개의 PUT 요청과 초당 5500개의 GET 요청을 처리한다.

- bucket/1/file => /1/
→ `bucket`과 `file` 사이에 있는 것이 접두사가 된다. 이 접두사를 갖는 파일은 초당 3500개의 PUT 요청과 초당 5500개의 GET 요청을 처리한다.
~~~

- If you spread reads across all four prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD  
→ 여러 접두사에 읽기 요청을 균등하게 분산하면 초당 22000개의 `GET/HAED` 요청을 처리할 수 있다. 

#### 1. S3 Performance
- Multi-Part upload:
~~~
- recommended for files > 100MB, must use for files > 5GB
→ 100MB가 넘는 파일은 멀티파일 업로드를 사용하는 것이 좋고 5GB가 넘는 파일은 반드시 사용해야 한다.

- Can help parallelize uploads (speed up transfers)
→ 멀티파트 업로드는 업로드를 병렬화하므로 전송 속도를 높여 대역폭을 최대화할 수 있다.
~~~

![image](https://user-images.githubusercontent.com/97398071/235167938-7409a687-bb99-47a7-afe4-ffd4d8f0de08.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- S3 Transfer Acceleration
~~~
- Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region
→ 업로드와 다운로드를 위한 S3 전송 가속화는 파일을 AWS 앳지 로케이션으로 전송해서 전송 속도를 높이고 데이터를 대상 리전에 있는 S3 버킷으로 전달한다. 엣지 로케이션은 리전보다 수가 많다. 현재에는 200개가 넘고 그 수는 계속 증가하고 있다.

- Compatible with multi-part upload
→ 전송 가속화를 멀티파트 업로드와 함께 사용할 수 있다.
~~~

![image](https://user-images.githubusercontent.com/97398071/235168434-edbdfc47-92a5-42c6-936d-950dc3ceac12.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

#### 2. S3 Performance – S3 Byte-Range Fetches
- `S3 바이트 범위 가져오기` 기능은 파일을 수신하고 파일을 읽는 가장 효율적인 방법 중 하나이다.

- Parallelize GETs by requesting specific byte ranges  
→ 파일에서 특정 바이트 범위를 가져와서 `GET` 요청을 병렬화한다.

- Better resilience in case of failures  
→ 특정 바이트 범위를 가져오는 데 실패한 경우에도 더 작은 바이트 범위에서 재시도하므로 실패에 대한 복원력이 높다.

- 첫 번째 사용 사례는 다운로드 속도를 높일 때이다.

![image](https://user-images.githubusercontent.com/97398071/235169224-e4fd999f-5112-42eb-bd0c-d5dd907ee26b.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- 두 번째 사용 사례는 파일의 일부 검색이다.  
→ 예를 들어 `S3`에 있는 파일의 첫 50바이트가 헤더라는 것을 알고 파일에 대한 정보를 안다면 첫 50바이트를 이용해서 헤더에 대한 바이트 범위 요청을 보내면 된다.

![image](https://user-images.githubusercontent.com/97398071/235169644-4c90b338-06e9-4ad1-851b-640037970706.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- `Amazon RDS PostgreSQL`을 사용하여 `S3`에 파일의 인덱스를 구축하려 합니다. 인덱스 구축을 위해서는 파일 콘텐츠 자체의 메타데이터를 포함하고 있는 `S3`에 있는 각 객체의 첫 250바이트를 읽는 작업이 필수적입니다. `S3` 버킷에는 총 `50TB`에 달하는 100,000개 이상의 파일이 포함되어 있습니다. 이 경우 인덱스를 구축하기 위한 효율적인 방법은 무엇일까요?  
→ `S3` 버킷을 트래버스하고, 첫 250바이트에 대한 바이트 범위 페치를 발행한 후, `RDS`에 해당 정보를 저장하는 애플리케이션 생성

### 10. S3 Select & Glacier Select
- Retrieve less data using SQL by performing server-side filtering  
→ `S3`에서 파일을 검색할 때 검색한 다음에 필터링하면 너무 많은 데이터를 검색하게 된다.

- Can filter by rows & columns (simple SQL statements)  
→ `SQL`문에서 간단히 행과 열을 사용해 필터링할 수 있다.

- Less network transfer, less CPU cost client-side  
→ 네트워크 전송이 줄어들기 때문에 데이터 검색과 필터링에 드는 클라이언트 측의 `CPU` 비용도 줄어든다.

### 11. S3 Batch Operations
- Perform bulk operations on existing S3 objects with a single request, example:  
→ 배치 작업은 단일 요청으로 기존 `S3` 객체에서 대량 작업을 수행하는 서비스이다.
~~~
- Modify object metadata & properties
→ 한 번에 많은 S3 객체의 메타데이터와 프로퍼티를 수정할 수 있다.

- Copy objects between S3 buckets
→ S3 버킷 간에 객체를 복사할 수 있다.

- Encrypt un-encrypted objects
→ 버킷 내 암호화되지 않은 모든 객체를 암호화할 수 있다.

- Modify ACLs, tags
→ ACL이나 태그를 수정할 수 있다.

- Restore objects from S3 Glacier
→ S3 Glacier에서 한 번에 많은 객체를 복원할 수 있다.

- Invoke Lambda function to perform custom action on each object
→ Lambda 함수를 호출해 S3 Batch Operations의 모든 객체에서 사용자 지정 작업을 수행할 수 있다.
~~~

- A job consists of a list of objects, the action to perform, and optional parameters  
→ 객체 목록에서 원하는 작업은 무엇이든지 수행 가능하다. 작업은 객체의 목록, 수행할 작업, 옵션 매개변수로 구성된다.

- S3 Batch Operations manages retries, tracks progress, sends completion notifications, generate reports …  
→ `S3 Batch Operations`를 사용하면 재시도 관리, 진행 상황 추적, 작업 완료 알림 송신, 보고서 생성 등이 가능하기에 직접 스크립팅하는 것보다 효율적이다 . 

- You can use S3 Inventory to get object list and use S3 Select to filter your objects  
→ 배치에 전달할 객체 목록은 `S3 Inventory`라는 기능을 사용해 객체 목록을 가져오고 `S3 Select`를 사용해 객체를 필터링한다. 

- `S3 Inventory`와 `S3 Select`를 사용해서 배치 작업에 포함하려는 필터링된 객체 목록을 얻은 후 `S3 Batch Operations`에 수행할 작업, 매개 변수와 함께 객체 목록을 전달한다. 
그러면 `S3` 배치가 작업을 수행하고 객체를 처리한다.

![image](https://user-images.githubusercontent.com/97398071/235172284-046d4106-bd52-4aa2-8ae2-f81170cda71a.png)

출처 → [AWS Certified Solutions Architect Slides v10](https://courses.datacumulus.com/downloads/certified-solutions-architect-pn9/)

- 주요 사용 사례에는 `S3 Inventory`를 사용해 암호화되지 않은 모든 객체를 찾은 다음 `S3 Batch Operations`를 사용해 한 번에 모두 암호화하는 것이 있다.
 
---
#### ▶ Reference
- [Ultimate AWS Certified Solutions Architect Associate SAA-C03](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/)
---